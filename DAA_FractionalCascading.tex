\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage[none]{hyphenat}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[noline,boxed]{algorithm2e}
\usepackage[skins]{tcolorbox}
\usepackage[nottoc,notlot,notlof]{tocbibind}


\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{\slshape \MakeUppercase{}}
\fancyhead[R]{\slshape}
\fancyfoot[C]{\thepage}
%\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

%\parindent 0ex
\setlength{\parindent}{4em}
\setlength{\parskip}{0em}
\renewcommand{\baselinestretch}{1.5}
\graphicspath{images/}
\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{0.5cm}
\Large{\textbf{Design and Analysis of Algorithms}}\\
\Large{\textbf{Theory Project}}\\
\vfill
\line(1,0){400}\\[1mm]
\huge{\textbf{Fractional Cascading}}\\[3mm]
\Large{\textbf{An algorithmic approach}}\\[1mm]
\line(1,0){400}\\
\vfill
By \\ 
IMT2019051 Mani Nandadeep Medicharla \\
IMT2019063 R Prasannavenkatesh \\
IMT2019525 Vijay Jaisankar \\
\end{center}
\end{titlepage}

\tableofcontents
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}

\section{Abstract}
        In this paper, we investigate the \textit{Fractional Cascading} technique in building range trees and for fast searching of an element in multiple arrays. In this venture, we introduce and examine \textit{Linear Search}, \textit{Binary Search}, \textit{Bridge Building} and \textit{Auxiliary lists}. We also look at some of the \textit{applications} of this technique, and suggest data structures for its efficient realisation. 

\section{Problem Statement}
\textit{Fractional cascading: }You are given an input of k ordered lists of numbers, each of size n as well as a query value x. The problem's output is
to return, for each list, True if the query value appears in the list and
False if it does not. For example, if the input is:
\begin{enumerate}[label=(\alph*)]
    \item List $L_1$: $[3,4,6]$
    \item List $L_2$: $[2,6,7]$
    \item List $L_3$: $[2,4,9]$
\end{enumerate}
and the query value is 4, then the expected output is [\textit{True},\textit{False},\textit{True}]. \\
\textbf{Give an algorithm to solve the fractional cascading problem.}



\section{Brute Force}
\subsection{Linear Search}

Linear search is the most basic search technique, wherein we sequentially compare each array element to the target element. In the worst case of the target element not coinciding with \textit{any} list element, the algorithm would reach the end of the list and we would report an unsuccessful search. \\
As each element is compared at most once, the time complexity is $O(n)$, where $n$ is the size of the list. \\ \\

This algorithm forms the basis of the simplest solution to our problem: We just run linear search on each of the $k$ lists. If we have $q$ queries, this takes $O(q \cdot k \cdot n)$ time, which is a lot and in real world situations, if $k$, $q$, and $n$ are even moderately large, the time taken would become astronomical. \\ 

\begin{tcolorbox}[blanker,width=(\linewidth-3.5cm)]
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{k arrays of size n and a query element x}
    \KwResult{Boolean array regarding whether element is present in the indiced array or not}
    
    \SetKwFunction{LinearSearch}{LinearSearch}
    \SetKwFunction{FMain}{Main}
    
    \SetKwProg{Fn}{Function}{:}{\KwRet \textit{False}}
    \Fn{\LinearSearch{Array,x}}
    {
        \For{$i\leftarrow 0$ \KwTo $n$}
        {
            \If{\textit{Array}[i] == x}
                {\KwRet \textit{True}\;}
        }
    }
    
    
    
    \SetKwProg{Fn}{Function}{:}{\KwRet 0}
    \Fn{\FMain}{
        output = [] \;
        \For{$i\leftarrow 0$ \KwTo $k$}
        {
            output.append(linearSearch(input[i],x);
        }
    }

% \caption{Brute Force method by doing k linear searches}
\end{algorithm}
\end{tcolorbox}

Note that, however, this approach does not take into account any relevant information given to us in the question which can speed up this algorithm. It is given that the \textit{lists are sorted}, so we can exploit this property and employ a faster searching technique to solve this problem in a better way: \textit{Binary Search}


%------------------------------------------------------------------------------%


\section{Improved Brute Force}
\subsection{Binary Search}



Binary search is another searching algorithm that works correctly only on sorted arrays. \\
It begins by comparing the target element with the element at the middle of the list. 
\begin{itemize}
    \item If they are equal, we have found the target in the list
    \item If the target is larger, and as the list is sorted, we must now turn our attention to the \textit{right half} of the list
    \item Similarly, if the target is smaller, we must focus on the \textit{left half} of the list.
\end{itemize}

In the worst case, Binary Search will take $O(\log n)$ comparisons, where n is the size of the list. \\ \\


To improve the performance of the Brute Force subroutine, we replace the Linear Search subroutine with the aforementioned Binary Search subroutine. If we have $q$ queries, this takes $O(q \cdot k \cdot \log n)$ time, which is certainly a lot better than the initial brute force algorithm, but can yet be improved further.

\begin{tcolorbox}[blanker,width=(\linewidth-3.5cm)]
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{k arrays of size n and a query element x}
    \KwResult{Boolean array regarding whether element is present in the indiced array or not}
    
    \SetKwFunction{BinarySearch}{BinarySearch}
    \SetKwFunction{FMain}{Main}
    
    \SetKwProg{Fn}{Function}{:}{\textbf{end Function}}
    \Fn{\BinarySearch{Array,x,left,right}}
    {
        \If{right $\geq$ left}{
            mid=$\frac{(left+right)}{2}$\;
            \If{Array[mid] == x}{
                \KwRet \textit{True}\;
            }
            \If{Array[mid] $>$ x}{
                BinarySearch(Array,x,left,mid-1)\;
            }
            \Else{
                BinarySearch(Array,x,mid+1,right)\;
            }  
        }
        \Else{
            \KwRet \textit{False}\;
        }
        
    }
    
    
    
    \SetKwProg{Fn}{Function}{:}{\KwRet 0}
    \Fn{\FMain}{
        output = [] \;
        \For{$i\leftarrow 0$ \KwTo $k$}
        {
            output.append(BinarySearch(input[i],x);
        }
    }

% \caption{Brute Force method by doing k linear searches}
\end{algorithm}
\end{tcolorbox}


%------------------------------------------------------------------------------%

\section{Bridge Building}
\subsection{Introduction to bridges}
A \textit{bridge} is a pointer from an element $a_i$ of $A_i$ to an element $a_j$ of $A_{i+1}$ where  $|a_i-a_j|$ is \textbf{small}, where $A$ is the list and $a$ represents an element of the array. By \textit{small}, we mean an element that is either of the same value, or with the smallest difference to the one considered as reference. \\ 

Once we locate the answer to a query in one array, we should be able to \textbf{follow a bridge} to a key that is close to the answer in the next array. \\
In the best case, we follow a bridge from the answer to the query in $A_i$ to a key in $A_{i+1}$ and then from there locate the answer in $A_{i+1}$, all in constant time. If we can do \textit{this},then once we have the answer in $A_1$ we can find the answer in the remaining $k-1$ sorted arrays in $O(k)$ time complexity. \\

From a technical standpoint, we \textit{implement} this method as follows: \\
For every element $e$ in the first array, give $e$ a pointer to the element with the \textit{same value} in the second array or if the value doesn't exist, the \textit{predecessor}. This is called bridge building between $A_i$ and $A_{i+1}$. Then, once we've found the item in the first array, we can just follow these pointers down in order to figure out where the item is in all the other arrays. To find the answer in $A_1$, we can just use a balanced binary search tree,thus making the overall time complexity of our algorithm $O(\log n + k)$ per query.

\begin{figure}[H]
    \centering
    \includegraphics[]{Images/Screenshot_2021-03-15 You could have invented fractional cascading Inside 245-5D.png}
    \caption{Process of bridge building for a given question}
    \label{fig:label}
\end{figure}





\subsection{Algorithm}
\begin{tcolorbox}[blanker,width=(\linewidth-3.5cm)]
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{k arrays of size n and a query element x}
    \KwResult{Boolean array regarding whether element is present in the indiced array or not}
    
    \SetKwFunction{BuildBridges}{BuildBridges}
    \SetKwFunction{FMain}{Main}
    
    \SetKwProg{Fn}{Function}{:}{\textbf{end Function}}
    \Fn{\BuildBridges{Array,x}}
    {
        \For{$i\leftarrow 0$ \KwTo $k-1$}{
            \For{$j\leftarrow 0$ \KwTo $n$}{
                Build bridge from Array[i][j] to Array[i+1][x] where $|$Array[i+1][y] - Array[i][j]$|$ is small. In this approach, if both predecessor and successor exists, then we take predecessor first.
            }
        }
    }
    
    
    \SetKwProg{Fn}{Function}{:}{\KwRet 0}
    \Fn{\FMain}{
        output = [] \;
        BuildBridges(input,x) \;
        output.append(BinarySearch(Array[i],x)) \;
        Once the element is found in the first array, follow the bridge path till the final array and append it to the output. \;
    }

% \caption{Brute Force method by doing k linear searches}
\end{algorithm}
\end{tcolorbox}
% \subsection{Proof of correctness}
\subsection{Shortcomings}

This method seems like a very interesting and efficient alternative to solve this problem. However, there are some glaring weaknesses to this approach, the most important one being the fact that certain classes of inputs render this method useless. \\
In particular, if a later list is completely \textit{in between} two elements of the first list, we have to redo the entire search, as the pointer pre-processing gives us no information that we didn't already know.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Screenshot_2021-03-15 fc dvi - notes08 pdf.png}
    \caption{An example where a later list is completely \textit{in between} two elements of the first list illustrated via bridge building}
    \label{fig:label}
\end{figure}


\textbf{Let's consider a simple example to elucidate this statement} \\
Consider the case where $k = 2$. Everything would be better if only we could guarantee that the first list contained the right elements to give you useful information about the second array. We could just merge the arrays, but if we did this in the general case we'd end up with a totally merged array of size kn, which is not so good if k is large.

Even if each time we find an answer in the sorted Array $A_i$, we follow the bridge pointer from it as well as from the key above it we are still left with the entire contents of catalog $A_{i+1}$ to search. This continues through the entire set of k catalogs.If we search each catalog by doing a linear scan from the point at which the bridge told us to begin,the total search time will be O(nk).Even if we build a balanced search tree over all elements in $A{i+1}$ that appear between two consecutive bridge pointers from $A_i$ the query time will still be O(klogn) which is similar to performing k binary searches to get the query element.



%------------------------------------------------------------------------------%
\section{Fractional Cascading}

\subsection{Intuition behind Fractional Cascading}
Because of the shortcomings of the \textit{Bridge Building} algorithm, i.e the case where every element of the "below array is in between all the elements of the "above" array.(\textbf{Note}: We say that array 1 is “above” array 2 and array 2 is “below” array 1. Therefore, an Array $j$ is below Array $i$ if $j > i$. In this case, we either have to do $k$ binary searches, or we have to merge all $n$ elements of the below array recursively and maintain bridges, to get the output. This will warrant a sub-optimal, extra time complexity of $O(k \cdot n)$, as we are going through \textit{each} array element iteratively to merge it.

To avoid this problem of merging all elements and getting $O(k \cdot n)$ time complexity, we start with the \textit{lowest} list in the sequence and select every $i$th element, where i = $\frac{1}{\alpha}$ and insert it into the array above it while still maintaining sorted order. \\
We then mark that element as \textit{promoted} and keep a pointer from it, to its original position in the bottom list. This operation of taking every $i$th element and \textit{promoting} it to the array above it is called \textit{cascading}, and since we are only promoting a fraction of the elements, the algorithm is called \textbf{fractional cascading}.

For selecting, $\alpha$, we can choose a variety of fractions, however $\alpha$ = $\frac{1}{2}$ seems most appropriate because, we will have to compare only 2 elements while searching, after prepocessing. This will also ensure that our time complexity stays low, and easily computable.


\subsection{Algorithmic Approach to Fractional Cascading}

Let the input be specified by $k$ $n$-element arrays, $A_1,A_2,\dots,A_k$, Let the query element value be $x$.
Let $M_1,M_2,\dots,M_k$ be the new \textit{merged arrays} such that $M_k = A_k$ and $\forall i < k$, $M_i$ is defined as the result of merging $M_i$ with every $\frac{1}{\alpha}$th element of $M_{i+1}$. \\ \\
As we're taking $\alpha$ = $\frac{1}{2}$, $M_i$ will be the result of merging $M_i$ with every alternate element of $M_{i+1}$. For every \textbf{cascaded element} of $M_i$ where i $<$ k, we keep two pointers from each element which are derived as follows:
\begin{itemize}
    \item If the element came from the same array, i.e, $A_i$, we keep a pointer to the nearest neighbouring elements from $M_{i+1}$
    \item if the element has been cascaded, we keep a pointer to the predecessor of the element in $M_i$ and also a bridge between $M_i$ and $M_{i+1}$ at the position where it is present in both of the arrays.
\end{itemize}
These pointers helps to find the position of the query element x in $A_i$ and also in the cascaded arrays below in $O(1)$ time. \\ \\
\textbf{Note}: Since we are merging every alternate element of the below list to the current list, we have 
$|M_i| = |A_i| +\frac{1}{2}|M_{i+1}|$, which in turn ensures that $|A_i| \leq 2n = O(n)$. \\

After we perform the aforementioned pre-processing, querying $x$ in all $k$ lists is done as follows.
First, we make a query for x in $M_1$ using a binary search in $O(\log n)$. Once we have found the position of $x$ in $M_1$, we use the \textit{cascaded pointers} to find the position of x in $M_2$. Generalising this step, once we found the position of x in $M_i$ where i $<$ k, we use the cascaded pointers to find the position of x in $M_{i+1}$. \\ \\
To find the location in $M_{i+1}$, we find the \textit{two neighbouring elements} in $M_{i+1}$ that came from $M_i$ using the pointers we had assigned during the pre-processing phase. Now, these elements will have \textit{exactly one element} between them in $M_{i+1}$. \textbf{Therefore, to find the exact location in $M_{i+1}$, we just have to do a simple comparison with only the intermediate element}. This is the significance of taking $\alpha$ = $\frac{1}{2}$ as we just have to perform only one comparison, which takes $O(1)$ time, and hence we can retrieve the location of $x$ in $A_i$ from its location in $M_i$ again in $O(1)$ time. \\ \\

Hence, the time to perform the pre-processing for fractional cascading is O(nk), the total search time per query is $O(k+log n)$ and, the total time taken by the Fractional Cascading algorithm is $O(q(k+logn))$ for $q$ queries, which is an astronomical improvement over the previous algorithms.



% Define new lists $L'_1, . . . , L'_k$ by $L'_k$ = $L_k$, and for $i < k$, let $L'_i$ be the result of merging $L'_{i}$ with every other element of $L'_{i+1}$ Note that $|L'_i| = |L_i| +\frac{1}{2}|L'_{i+1}|$, so $|Li| \leq 2n = O(n).$. 
% For each $i < k$, keep two pointers from each element. If the element came from $L_i$, keep a pointer to the two neighboring elements from $L'_{i+1}$, and vice versa. These pointers allow us to take information of our placement in $L'_i$ and in O(1) turn it into information about our placement in $L_i$ and our placement in half of $L'_{i+1}$.
%Now to make a query for x in all k lists is quite straightforward. 
%First, query $L'_1$ for the location of x with a binary search. Now to find the location of x in $L'_{i+1}$ from the location of x in $L'_i$, find the two neighboring elements in $L'{i+1}$ that came from $L'_i$ using the extra pointers. Then these elements have exactly one element between them in $L'_{i+1}$. To find our actual location in $L'_{i+1}$, we simply do a comparison with that intermediate element. This allows us to turn the information about
%x’s location in $L'_i$ into information about x’s location in $L'_{i+1}$ in O(1) time, and we can retrieve x’s location in $L_i$ from its location in $L'_i$ in O(1), and thus we can query for x in all k lists in O(k + log n) time.


\subsection{PseudoCode}
\begin{tcolorbox}[blanker,width=(\linewidth-3.5cm)]
\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{k arrays of size n and a query element x}
    \KwResult{Boolean array regarding whether element is present in the indiced array or not}

    \SetKwFunction{FFractionalCascading}{Fractional\_Cascading}
    \SetKwProg{Fn}{Function}{:}{\KwRet output}
    \Fn{\FFractionalCascading}{
        output = [] \;
        MergedArrays = [] \;
        % // Insert all elements of the last array at position 0 of MergedArrays % \;
        MergedArrays = MergedArrays.insert(0,all elements of the last array) \;
        MergedArrays = MergedArrays.insert(0,merge the below array with the above array by only taking alternate elements of the below array) \;
        %Generate the boundry case predecessors and successors; $- \infty$ and $+ \infty$ \;
        For every element assign a two sized array to hold the pointers to the locations. \;
        For every element in the merged arrays, assign locations based on the presence of that particular element in $A_i$ and $M_{i+1}$.If the element came from the same array, i.e, $A_i$, we keep a pointer to the nearest neighbouring elements from $M_{i+1}$. If the element has been cascaded, we keep a pointer to the predecessor of the element in $M_i$ and also a bridge between $M_i$ and $M_{i+1}$ at the position where it is present in both of the arrays.\;
        
        The gist of the algorithm is that we check for the position of the target element in the merged array, then we follow the pointers down to get the positions of the predecessors of the said target element in all of the k arrays. Let's call this array $positions_k$ \;
        The last step is to scan through $positions_k$, and see if the respective predecessor is actually the given target, or not. This generates the $[True,False]$ format given in the question and append it to the output array.
    }
    
    

% \caption{Brute Force method by doing k linear searches}
\end{algorithm}
\end{tcolorbox}



\subsection{Example}
\begin{enumerate}[label=(\alph*)]
    \item List $L_1$: $[3,4,6]$
    \item List $L_2$: $[2,6,7]$
    \item List $L_3$: $[2,4,9]$
\end{enumerate}

\begin{figure}[!h]
    \centering
    \includegraphics[]{Images/Screenshot_2021-03-15 Fractional Cascading(1).png}
    \caption{Fractional cascading pre processing for the given question}
    \label{fig:label}
\end{figure}
    


The figure gives the final MergedArrays and we can also see how the elements are cascaded. Every cascaded element has a pointer to it's predecessor which was initally present in the same array $A_i$ and also has a bridge between merged arrays $M_i$ and $M_{i+1}$ depending on the location of that particular element in both the arrays. The green and pink bridges demonstrate this fact. Also the elements that are present in both $A_i$ and $M_i$ has a pointer to the nearest cascaded element as shown by the purple pointers in the figure. 



\begin{figure}[H]
    \centering
    \includegraphics[]{Images/Screenshot_2021-03-15 Fractional Cascading(2).png}
    \caption{Fractional cascading output(search) for the given question}
    \label{fig:label}
\end{figure}

Here, the search query element, x = 4. So we start at $M_1$ and search for 4 using a binary search which will in turn return the first 4 in the array, i.e, the element present in index 1. Now we will follow this index's pointer to the nearest cascaded element which is 4 present at index 2. However, this is a cascaded element from the below arrays and hence will have a bridge to the below arrays. So the algorithm follows the bridge and goes to index 1 of $M_2$. However, this elements is also cascaded, so we check whether the previous element is equal to 4 or not. The previous element in this case is 2 present at index 0 of $M_2$ which is not equal to the query element. So we again go to the nearest cascaded element via 2's pointer and reach the next merged array which is $M_3$. Here the bridge's endpoint is the query element and it will return true and the algorithm is terminated as we have reached the lowest array. Hence the output will be [\textit{True},\textit{False},\textit{True}].

\pagebreak

\subsection{Proof of Correctness}
To prove that our Fractional Cascading algorithm is correct, we perform the following steps.
\begin{itemize}
    \item We look at an other simpler algorithm and prove its correctness, Let's call this algorithm X.
    \item We show that our algorithm and X produce the same output. This forms the proof of correctness of our algorithm.
\end{itemize}

Note that we already have a candidate for algorithm X; Our improved brute force algorithm which uses binary search.

%proof of correctness of binary search.

We will follow the concept of universal generalization here; lets consider the $r^{th}$ array of our input sequence consisting of k arrays of size $n$. Lets also consider a target element $x$. Clearly there are two cases
\begin{itemize}
    \item x $\in$ array[r] lets call this situation 1.
     \item x $\notin$ array[r] lets call this situation 2.
\end{itemize}

Lets start with situation 1, 


Now there are two possibilities here when our control reaches array[r] 
\begin{itemize}
    \item we find element x and it is not cascaded from below then we can say that it return true which is exactly what algorithm X returns in the same situation .
    \item we find element x but it is cascaded from below so, we look at its neighbours now, as x belongs to array[r], native x will be one of the neighbours of the previous element.Now this case transforms into the previous case and we can return true.
\end{itemize}
Now lets continue with situation 2.
Now there are two possibilities here when our control reaches array[r] 

\begin{itemize}
    \item we find element x but it is cascaded from below.So we look at its nearest non cascaded neighbours which are guaranteed not to be x.So we can return false.
    \item we find an element which is not equals to x. Now  regardless of whether its cascaded or not when we traverse to the pointers we are guaranteed not to find x so we return false.
\end{itemize}

%------------------------------------------------------------------------------%


\section{Applications}
This technique has various applications in numerous fields. \\
\begin{enumerate}
    \item Computational Geometry
    \begin{itemize}
        \item Half-Range Plane Reporting
        \item Explicit Searching 
        \item Point Location
    \end{itemize}
    \item Networks
    \begin{itemize}
        \item Fast Packet filtering in internet routers.
        \item data distribution and retrieval in sensor networks
    \end{itemize}
    \item Linear Range Queries
        \begin{itemize}
            \item As an accompaniment to \textit{Segment Trees}
        \end{itemize}
\end{enumerate}


%------------------------------------------------------------------------------%

\pagebreak 
\begin{thebibliography}{}

\bibitem{Fractional cascading notes by Prof Prof.RobertoTamassia}
C.S.252 ``Prof.RobertoTamassia'' ComputationalGeometry Sem.II, 1992-1993
\\
\texttt{http://cs.brown.edu/courses/cs252/misc/resources/lectures/pdf/notes08.pdf}


\bibitem{MIT OCW Notes on Fractional Cascading}
6.851:Advanced Data Structures Spring 2012 ``Prof.Erik Demaine'' \\
\href{https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/calendar-and-notes/MIT6_851S12_L3.pdf}{Link to MIT Lecture notes by Prof Erik Demaine on the topic Advanced Data Structures}


\bibitem{Fractional Cascading Demo}
Fractional Cascading webpage created by Ravi Sinha alias ravix339-zz \\
\texttt{https://ravix339.github.io/FractionalCascading/index.html} \\
\texttt{https://ravix339.github.io/FractionalCascading/Demo.html}

\bibitem{Blog by Edward Z. Yang}
A blog about fractional cascading and bridge building by Edward Z. Yang
\\\texttt{http://blog.ezyang.com/2012/03/you-could-have-invented-fractional-cascading/}


\end{thebibliography}

\end{document}
